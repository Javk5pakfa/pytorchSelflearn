{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Positional Encoding with inhomogeneous spacing\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, times):\n",
    "        device = x.device\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        pe = torch.zeros(batch_size, seq_len, self.d_model, device=device)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.d_model, 2, device=device).float() * (\n",
    "                        -np.log(10000.0) / self.d_model))\n",
    "\n",
    "        pe[:, :, 0::2] = torch.sin(times.unsqueeze(-1) * div_term)\n",
    "        pe[:, :, 1::2] = torch.cos(times.unsqueeze(-1) * div_term)\n",
    "\n",
    "        return x + pe\n",
    "\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward,\n",
    "                 output_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead,\n",
    "                                                    dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers,\n",
    "                                                         num_layers)\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.decoder = nn.Linear(d_model, output_dim)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, times):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != src.size(1):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(src.size(1)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src)\n",
    "        src = self.pos_encoder(src, times)\n",
    "        output = self.transformer_encoder(src.transpose(0, 1), self.src_mask)\n",
    "        output = self.decoder(output.transpose(0, 1))\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(\n",
    "            mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "# Training function\n",
    "#The model is looking at the first 99 points (input_seq = val_data[i:i+1, :-1, :]) and \n",
    "#predicting the next 99 points (prediction = model(input_seq, input_times)).\n",
    "def train(model, train_data, train_times, val_data, val_times, num_epochs,\n",
    "          batch_size, lr):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i in range(0, train_data.shape[0], batch_size):\n",
    "            batch_data = train_data[i:i + batch_size]\n",
    "            batch_times = train_times[i:i + batch_size]\n",
    "            input_seq = batch_data[:, :-1, :]\n",
    "            input_times = batch_times[:, :-1]\n",
    "            target_seq = batch_data[:, 1:, :]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq, input_times)\n",
    "            loss = criterion(output, target_seq)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss = total_loss / (train_data.shape[0] // batch_size)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(val_data[:, :-1, :], val_times[:, :-1])\n",
    "            val_loss = criterion(val_output, val_data[:, 1:, :])\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    return train_losses, val_losses"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data Generation with inhomogeneous spacing\n",
    "def generate_data(batch_size, seq_length, num_features, min_gap=0.01,\n",
    "                  max_gap=0.5):\n",
    "    data = np.zeros((batch_size, seq_length, num_features))\n",
    "    times = np.zeros((batch_size, seq_length))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        t = 0\n",
    "        for j in range(seq_length):\n",
    "            times[i, j] = t\n",
    "            gap = np.random.uniform(min_gap, max_gap)\n",
    "            t += gap\n",
    "\n",
    "        times[i] /= times[i, -1]  # Normalize to [0, 1] range\n",
    "\n",
    "        for k in range(num_features):\n",
    "            freq = np.random.rand() * 2 + 0.1\n",
    "            phase = np.random.rand() * 2 * np.pi\n",
    "            amplitude = np.random.rand() * 2\n",
    "            if k % 2 == 0:\n",
    "                data[i, :, k] = amplitude * np.sin(freq * times[i] + phase)\n",
    "            else:\n",
    "                data[i, :, k] = amplitude * np.cos(freq * times[i] + phase)\n",
    "\n",
    "    noise = np.random.randn(*data.shape) * 0.1\n",
    "    data += noise\n",
    "\n",
    "    return torch.FloatTensor(data), torch.FloatTensor(times)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# function to plot example data\n",
    "def plot_example_data(data, times, num_examples=3):\n",
    "    num_features = data.shape[2]\n",
    "    fig, axs = plt.subplots(num_examples, 1, figsize=(12, 4 * num_examples),\n",
    "                            sharex=True)\n",
    "    fig.suptitle('Example Time Series with Inhomogeneous Spacing', fontsize=16)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        example_data = data[i]\n",
    "        example_times = times[i]\n",
    "\n",
    "        for j in range(num_features):\n",
    "            axs[i].plot(example_times, example_data[:, j],\n",
    "                        label=f'Feature {j + 1}', alpha=0.7)\n",
    "            axs[i].scatter(example_times, example_data[:, j],\n",
    "                           label=f'Feature {j + 1}', alpha=0.7)\n",
    "\n",
    "        axs[i].set_ylabel('Value')\n",
    "        #axs[i].legend()\n",
    "        axs[i].grid(True)\n",
    "\n",
    "    axs[-1].set_xlabel('Time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data\n",
    "batch_size = 1000\n",
    "seq_length = 100\n",
    "num_features = 1\n",
    "data, times = generate_data(batch_size, seq_length, num_features)\n",
    "\n",
    "# Plot example data\n",
    "plot_example_data(data, times)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def multi_step_predict(model, input_seq, input_times, prediction_window):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_seq, input_times)\n",
    "        return output[:, -prediction_window:, :]\n",
    "\n",
    "\n",
    "# Modified function to plot multiple predictions\n",
    "def plot_multiple_predictions(model, val_data, val_times, num_examples=3,\n",
    "                              input_window=80, prediction_window=20):\n",
    "    for i in range(num_examples):\n",
    "        input_seq = val_data[i:i + 1, :input_window, :]\n",
    "        input_times = val_times[i:i + 1, :input_window]\n",
    "        target_seq = val_data[i:i + 1,\n",
    "                     input_window:input_window + prediction_window, :]\n",
    "        target_times = val_times[i:i + 1,\n",
    "                       input_window:input_window + prediction_window]\n",
    "\n",
    "        prediction = multi_step_predict(model, input_seq, input_times,\n",
    "                                        prediction_window)\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        for j in range(input_seq.shape[2]):\n",
    "            plt.subplot(input_seq.shape[2], 1, j + 1)\n",
    "            plt.plot(input_times[0].numpy(), input_seq[0, :, j].numpy(),\n",
    "                     label='Input', alpha=0.7)\n",
    "            plt.plot(target_times[0].numpy(), target_seq[0, :, j].numpy(),\n",
    "                     label='True', alpha=0.7)\n",
    "            plt.scatter(target_times[0].numpy(), prediction[0, :, j].numpy(),\n",
    "                        label='Predicted', alpha=0.7, c='teal')\n",
    "            plt.legend()\n",
    "            plt.title(f'Example {i + 1}, Feature {j + 1}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training parameters\n",
    "num_epochs = 20\n",
    "train_batch_size = 32\n",
    "lr = 0.001\n",
    "input_window = 80\n",
    "prediction_window = 20\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses = train(model, train_data, train_times, val_data,\n",
    "                                 val_times, num_epochs, train_batch_size, lr,\n",
    "                                 input_window, prediction_window)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
